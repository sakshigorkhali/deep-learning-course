{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc182ebc-0256-47e2-a3f2-4a2a7617d128",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Preprocessing Image Data in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba795966-8121-4900-8783-6245ec4836a9",
   "metadata": {},
   "source": [
    "Before feeding raw images into a deep learning model, it's essential to transform them into a format that's easier for the model to interpret. This process is called **image preprocessing**. In this tutorial, we’ll walk through several common preprocessing steps using a small sample from the well-known **Kaggle Dogs vs. Cats** dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "+ Understand the importance of image preprocessing in deep learning workflows.\n",
    "+ Load and organize image data using `image_dataset_from_directory()`.\n",
    "+ Resize and center-crop images to ensure uniform input dimensions.\n",
    "+ Normalize pixel values for improved model performance.\n",
    "\n",
    "## Prerequisites\n",
    "Before getting started, you should have:\n",
    "+ Basic knowledge of Python programming (e.g., functions, loops, packages).\n",
    "+ Familiarity with `NumPy` and `Matplotlib` for array manipulation and plotting.\n",
    "+ A Python environment (version 3.x) with `tensorflow`, `keras`, `Pillow`, and `matplotlib` installed.\n",
    "+ Some exposure to image data and the concept of tensors is helpful, but not required.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The images are stored in a directory called `\"train\"`, with a subdirectory for cat images called `\"cat\"`, and another for dog images called `\"dog\"`. Let's preview a couple of the images to get an idea what they look like, starting with one of the cat images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d430a2-4680-4cf9-9e0d-b78381ca07f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdfdea85-c48d-4349-91ff-ad35198e8cf1",
   "metadata": {},
   "source": [
    "Next, let's look at one of the dog images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63d513-5517-42bc-8824-79e7cced7f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02379368-1876-4b3d-9f5d-f080176acb0d",
   "metadata": {},
   "source": [
    "## 1. Load and Batch the Images\n",
    "Now that we've seen a few examples of the images we'll be working with, let's work on loading them into a format that will be useful for deep learning. When training a convolutional neural network, images are rarely processed one at a time. Instead, they are grouped into batches. A **batch** is a collection of images combined into a single tensor for efficient computation, particularly on GPUs. All images in a batch must have the same dimensions and number of channels. \n",
    "\n",
    "Keras provides a utility function called `image_dataset_from_directory()` that allows us to create a pipeline that automatically turns images on disk into batches of **preprocessed tensors**. We're going to use it to load our data with the following configuration:\n",
    "+ `labels = \"inferred\"`: Looks at each subfolder name under the \"train\" directory and assigns that folder’s index as the label.\n",
    "+ `label_mode = \"int\"`: Stores the labels as integers. Other options include `\"categorical\"` for one-hot vectors, `\"binary\"` for the single float values of 0.0 or 1.0, or `\"None\"` for no labels, which is useful during inference. \n",
    "+ `batch_size = 6`: Creates batches of 6 images each. When training a convolutional neural network (CNN), images are rarely processed one at a time. Instead, they are grouped into **batches** (a collection of images combined into a single tensor for efficient computation, particularly on GPUs).\n",
    "+ `seed = 1234`: Randomizes the order in which files are read in a reproducible way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391aab94-acd7-4313-a9a6-a9c439f67265",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> For a primer on tensors and how they're used in deep learning, watch the LinkedIn Learning course <b>\"Deep Learning with Python: Foundations\"</b>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010cce1-2506-4c26-bfc2-5a4805a8568b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e0b9d37-05c4-4d5a-be73-3a5c51a69eb9",
   "metadata": {},
   "source": [
    "Let's take a look at the imported images to see what they look like. We start by defining a reusable helper function `display_images()` that will help us display multiple images side-by-side for quick visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d603c5b-4d80-474a-bea3-6a57270fd7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9655fa6-600a-4d9c-983a-474803804a30",
   "metadata": {},
   "source": [
    "Then, we preview the imported and resized images using the `display_images()` function we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d037b1-cf41-4613-9650-2eff6dc2dd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86df69fd-3227-4c72-ac4f-c3e8fd25c6a8",
   "metadata": {},
   "source": [
    "Now we see that each of the images have been resized to 256 by 256 pixels, which is the deafult image size used by `image_dataset_from_directory()` utility function. Since these are color images, each batch of 6 images would be stored as a rank-4 tensor with the shape (6, 256, 256, 3).\n",
    "\n",
    "**Note:** If you see a message at the end of the output, it isn't an error. It's just TensorFlow’s way of saying \"I've delivered all the data you asked for.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0481f-03dc-48e6-97e0-04da19284a2c",
   "metadata": {},
   "source": [
    "## 2. Center-Crop and Resize\n",
    "Many well-known convolutional neural network architectures use standard input sizes. For example, VGG and ResNet typically expect images of size 224 × 224 pixels, while Inception networks use 299 × 299. As a result, before we feed images to these models, they have to be adjusted to the expected size. Sometimes, this may also involve center-cropping to a square or desired aspect ratio, before resizing to avoid distortion. For example, to center-crop and resize our images to 224 x 224 pixels, we set `crop_to_aspect_ratio = True` and specify `image_size = (224, 224)`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e48b8-2d18-4aef-a7c9-59ed7d2af948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9e7b0c-5b9c-4398-8c26-efe2f3719948",
   "metadata": {},
   "source": [
    "Let's preview the images to see what impact center-cropping and resizing has on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83469bf-1e9d-4531-9fc4-a5c105225900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dafdb86a-22eb-463a-a340-7e9d02d69272",
   "metadata": {},
   "source": [
    "Comparing these outputs to the previous ones, we see that some of the non-cropped images (previous ones) were stretched to fit the 256 by 256 default aspect ratio, while the cropped images are trimmed, then resized to preserve the original proportions of the cat or dog in each image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115c854-5a86-49cc-aa71-cc2726547c81",
   "metadata": {},
   "source": [
    "## 3. Normalize Pixel Values\n",
    "Pixel values in 8-bit encoded images range from 0 to 255. Deep learning models (such as CNNs) perform better when input values are scaled to a smaller range of values. To accommodate this, we often need to normalize the pixel values so they fall within 0 and 1. There are several ways to do this. One approach is to define a preprocessing pipeline using **Keras preprocessing layers**. Each time an image passes through this layer, it will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef633fa-0b20-4be4-9294-80a69e803d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc819c6-080f-4430-bf6f-3c03de6d99eb",
   "metadata": {},
   "source": [
    "Now, let's pass all the images through the preprocessing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6f470-80f4-454a-bbe9-675991c6dfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cacc4bc4-4b6b-47e6-a46e-9a4e5c25e79f",
   "metadata": {},
   "source": [
    "To confirm the pixel ranges after normalization, let’s peek at the minimum and maximum pixel values of the images before and after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd44d11-474b-420a-b9ac-6b6c412cf370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "685842df-3394-48be-b7ff-f0807d53a85e",
   "metadata": {},
   "source": [
    "This verifies that the pixel values of each image has been scaled correctly to fall within 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ee8b3-2959-405a-88d5-1201f677133b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> Instead of resizing images during the load process, we can resize images as part of our preprocessing pipeline by including a resizing layer (e.g., <samp>layers.Resizing(299, 299)</samp>).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90053d70-5324-4242-a783-69d65cbc4c7d",
   "metadata": {},
   "source": [
    "In this tutorial, we explored the key preprocessing steps required to prepare raw image data for use in deep learning models. You learned how to load, resize, crop, and normalize images to ensure consistency across input data. These steps help improve both the efficiency and performance of your models. In the next lesson, we’ll take things further by applying **image augmentation** techniques to expand our dataset and enhance model generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
