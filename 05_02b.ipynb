{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed2254f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Evaluating a Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Deep learning has transformed machine learning, driving breakthroughs in computer vision, natural language processing, and speech recognition. In this tutorial, we'll explore the crucial step of evaluating a deep learning model in Python, focusing on metrics that reveal different aspects of model performance. We'll learn to measure predictive accuracy, interpret a confusion matrix, and assess model precision, recall, and the F1-score. These evaluation metrics are essential for understanding a model's strengths and highlighting areas for improvement.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Know how to load a previously trained Keras model.\n",
    "+ Know how to compute and visualize a confusion matrix.\n",
    "+ Understand the structure and components of a confusion matrix.\n",
    "+ Know how to calculate key performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "+ Be able to differentiate between precision and recall, understanding the trade-offs between the two.\n",
    "+ Understand the importance of the F1-score as a balanced metric between precision and recall.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with fundamental machine learning concepts (datasets, training/testing, overfitting).\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, `numpy`, `matplotlib`, `seaborn`, and `sklearn` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import the Model and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "To start, we import the deep learning model we trained in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('nnet_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3ec92",
   "metadata": {},
   "source": [
    "Next, we import the test data, which is new unseen data that our model will be evaluated against. \n",
    "\n",
    "Recall that the test data is part of the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28x28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. \n",
    "\n",
    "To get the test images and test labels, we call the `keras.datasets.mnist.load_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Note that the function returns both the training and test datasets. We'll ignore the training data for this tutorial.\n",
    "\n",
    "Our deep learning model expects the test images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc258bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Predictive Accuracy</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1df9bb",
   "metadata": {},
   "source": [
    "After training, it's crucial to assess how well a model is expected to perform against new, unseen data. There are several metrics that provide us with this information. **Predictive accuracy** is one of them. Predictive accuracy is a straightforward metric and is defined as the ratio of correctly predicted instances to the total number of instances in the data. It is a common metric for classification tasks.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "To get the predictve accuracy of our model, we first get the predicted probabilities for each of the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778186fd",
   "metadata": {},
   "source": [
    "Then we convert the probabilities to class labels by selecting the class label with the highest predicted probability using the `argmax()` function from the `numpy` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88671579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d7a3b",
   "metadata": {},
   "source": [
    "We do the same thing for the true test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = np.argmax(test_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b5a84",
   "metadata": {},
   "source": [
    "With the predicted and true class values, we can use the `accuracy_score()` function provided by `sklearn.metrics` to calculate the accuracy of the model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"The Accuracy of the model on the test images is {accuracy:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d766b35",
   "metadata": {},
   "source": [
    "A predictive accuracy of 0.9801 means that 98.01% of the predictions made by the model were correct. In other words, out of the 10,000 test images in the data, the model correctly predicted the labels for 9,801 of them, while 199 were misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96308dad",
   "metadata": {},
   "source": [
    "Predictive accuracy provides an overall view of how well a model performs on the entire test dataset, giving a general sense of its effectiveness. However, this metric alone may not always reveal important details about the model's weaknesses. To gain deeper insights, it can be useful to examine specific instances where the model made errors.\n",
    "\n",
    "One way to do this is by identifying and analyzing misclassified examples (i.e. those cases where the model's predictions differ from the true labels). By visualizing a few of these misclassified images, along with their actual and predicted labels, we can better understand patterns in the model's mistakes and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "misclassified_images = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "for i in misclassified_images[:5]:\n",
    "    plt.imshow(test_images[i].reshape(28, 28), cmap = 'gray')\n",
    "    plt.title(f'True Label: {true_classes[i]}, Predicted: {predicted_classes[i]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614b47d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Confusion Matrix</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d143d",
   "metadata": {},
   "source": [
    "A **confusion matrix** provides additional insight into the performance of a model by summarizing how well the model does in classifying each label. This can help us, for instance, identify if there are digits which are commonly misclassified by the model.\n",
    "\n",
    "To create a confusion matrix for our model, we first use the `confusion_matrix()` function from `sklearn.metrics` to calculate the values of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29155a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "683a589b",
   "metadata": {},
   "source": [
    "Then we visualize the matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dae3d2a",
   "metadata": {},
   "source": [
    "The diagonal elements of the confusion matrix represent the number of correct predictions for each label, while the off-diagonal elements represent misclassifications.\n",
    "\n",
    "Overall, the model performs exceptionally well in correctly classifying most images. However, we do observe that the model occasionally confuses '4' for '9' (19 instances) and '7' for '9' (17 instances). While these are relatively minor errors compared to the 956 correctly classified images of the number '4' and the 998 correctly classified images of the number '7', it may be worth investigating further to identify potential areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
