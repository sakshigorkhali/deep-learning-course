{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed2254f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Evaluating a Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Deep learning has transformed machine learning, driving breakthroughs in computer vision, natural language processing, and speech recognition. In this tutorial, we'll explore the crucial step of evaluating a deep learning model in Python, focusing on metrics that reveal different aspects of model performance. We'll learn to measure predictive accuracy, interpret a confusion matrix, and assess model precision, recall, and the F1-score. These evaluation metrics are essential for understanding a model's strengths and highlighting areas for improvement.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Know how to load a previously trained Keras model.\n",
    "+ Know how to compute and visualize a confusion matrix.\n",
    "+ Understand the structure and components of a confusion matrix.\n",
    "+ Know how to calculate key performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "+ Be able to differentiate between precision and recall, understanding the trade-offs between the two.\n",
    "+ Understand the importance of the F1-score as a balanced metric between precision and recall.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with fundamental machine learning concepts (datasets, training/testing, overfitting).\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, `numpy`, `matplotlib`, `seaborn`, and `sklearn` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import the Model and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "To start, we import the deep learning model we trained in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('nnet_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3ec92",
   "metadata": {},
   "source": [
    "Next, we import the test data, which is new unseen data that our model will be evaluated against. \n",
    "\n",
    "Recall that the test data is part of the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28x28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. \n",
    "\n",
    "To get the test images and test labels, we call the `keras.datasets.mnist.load_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Note that the function returns both the training and test datasets. We'll ignore the training data for this tutorial.\n",
    "\n",
    "Our deep learning model expects the test images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc258bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Predictive Accuracy</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1df9bb",
   "metadata": {},
   "source": [
    "After training, it's crucial to assess how well a model is expected to perform against new, unseen data. There are several metrics that provide us with this information. **Predictive accuracy** is one of them. Predictive accuracy is a straightforward metric and is defined as the ratio of correctly predicted instances to the total number of instances in the data. It is a common metric for classification tasks.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "To get the predictve accuracy of our model, we first get the predicted probabilities for each of the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778186fd",
   "metadata": {},
   "source": [
    "Then we convert the probabilities to class labels by selecting the class label with the highest predicted probability using the `argmax()` function from the `numpy` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88671579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d7a3b",
   "metadata": {},
   "source": [
    "We do the same thing for the true test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = np.argmax(test_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b5a84",
   "metadata": {},
   "source": [
    "With the predicted and true class values, we can use the `accuracy_score()` function provided by `sklearn.metrics` to calculate the accuracy of the model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"The Accuracy of the model on the test images is {accuracy:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d766b35",
   "metadata": {},
   "source": [
    "A predictive accuracy of 0.9801 means that 98.01% of the predictions made by the model were correct. In other words, out of the 10,000 test images in the data, the model correctly predicted the labels for 9,801 of them, while 199 were misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96308dad",
   "metadata": {},
   "source": [
    "Predictive accuracy provides an overall view of how well a model performs on the entire test dataset, giving a general sense of its effectiveness. However, this metric alone may not always reveal important details about the model's weaknesses. To gain deeper insights, it can be useful to examine specific instances where the model made errors.\n",
    "\n",
    "One way to do this is by identifying and analyzing misclassified examples (i.e. those cases where the model's predictions differ from the true labels). By visualizing a few of these misclassified images, along with their actual and predicted labels, we can better understand patterns in the model's mistakes and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "misclassified_images = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "for i in misclassified_images[:5]:\n",
    "    plt.imshow(test_images[i].reshape(28, 28), cmap = 'gray')\n",
    "    plt.title(f'True Label: {true_classes[i]}, Predicted: {predicted_classes[i]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614b47d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Confusion Matrix</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d143d",
   "metadata": {},
   "source": [
    "A **confusion matrix** provides additional insight into the performance of a model by summarizing how well the model does in classifying each label. This can help us, for instance, identify if there are digits which are commonly misclassified by the model.\n",
    "\n",
    "To create a confusion matrix for our model, we first use the `confusion_matrix()` function from `sklearn.metrics` to calculate the values of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29155a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a589b",
   "metadata": {},
   "source": [
    "Then we visualize the matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae3d2a",
   "metadata": {},
   "source": [
    "The diagonal elements of the confusion matrix represent the number of correct predictions for each label, while the off-diagonal elements represent misclassifications.\n",
    "\n",
    "Overall, the model performs exceptionally well in correctly classifying most images. However, we do observe that the model occasionally confuses '4' for '9' (19 instances) and '7' for '9' (17 instances). While these are relatively minor errors compared to the 956 correctly classified images of the number '4' and the 998 correctly classified images of the number '7', it may be worth investigating further to identify potential areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43625b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>4. Precision</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473c067",
   "metadata": {},
   "source": [
    "**Precision** is a metric that measures the accuracy of the positive predictions made by a model. Specifically, it quantifies how many of the instances that the model predicted as positive are actually true positives. \n",
    "\n",
    "Precision answers the question: *Of all the instances the model classified as positive, how many were actually correct?*\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)}+\\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "+ **True Positives (TP):** Cases where the model correctly predicted the positive class.\n",
    "+ **False Positives (FP):** Cases where the model predicted the positive class but the actual class was negative (i.e., the model was wrong).\n",
    "\n",
    "High precision means that when the model predicts a class, it is very likely to be correct. This is especially important in situations where false positives are costly. For example:\n",
    "\n",
    "+ **Spam detection:** If the model wrongly classifies legitimate emails as spam (false positives), important emails may be missed.\n",
    "+ **Medical diagnosis:** If a model incorrectly predicts that a healthy person has a disease (false positive), it can lead to unnecessary stress, treatment, or procedures.\n",
    "\n",
    "In these cases, minimizing false positives is crucial, so high precision is desired.\n",
    "\n",
    "To calculate the precision score of our model for each class of the test data, we use the `precision_score()` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_per_class = precision_score(true_classes, predicted_classes, average = None)\n",
    "for label, precision in enumerate(precision_per_class):\n",
    "    print(f'Precision for digit {label}: {precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5654216",
   "metadata": {},
   "source": [
    "Let's interpret the results using the first line of the output as an example. \n",
    "\n",
    "The precision for the digit '0' in this case is 0.9928 or 99.28%. This means that, out of all the instances where the model predicted the digit '0', 99.28% of those predictions were correct. The remaining 0.72% were false positives (i.e. instances where the model incorrectly predicted other digits as '0')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d572d",
   "metadata": {},
   "source": [
    "Note that the precision metric for digit '0' can also be derived from the confusion matrix, where **TP** is **962** and **FP** is **7**, which is the sum of the misclassifications in the same column:\n",
    "\n",
    "<img src=\"precision.jpg\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321052b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>5. Recall</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7bdbff",
   "metadata": {},
   "source": [
    "**Recall** (also known as sensitivity or true positive rate) is a metric that measures how well a model can identify actual positive instances. It quantifies how many of the true positive instances were correctly classified by the model. \n",
    "\n",
    "Recall answers the question: *Of all the actual positive instances, how many did the model correctly identify?*\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)}+\\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "+ **True Positives (TP):** Cases where the model correctly predicted the positive class.\n",
    "+ **False Negatives (FN):** Cases where the model should have predicted the positive class but did not (i.e., it missed actual positives).\n",
    "\n",
    "High recall means the model correctly identifies most of the positive cases. This is particularly important in cases where missing a positive instance (false negatives) has serious consequences. Some examples include:\n",
    "\n",
    "+ **Medical diagnosis:** If a model misses detecting a disease (false negative), the consequences for the patient could be serious.\n",
    "+ **Fraud detection:** If a fraudulent transaction goes undetected (false negative), the financial damage could be significant.\n",
    "\n",
    "In these situations, high recall is critical, as it minimizes the chance of missing positive instances (e.g., sick patients, fraudulent transactions).\n",
    "\n",
    "To calculate the recall score of our model for each class of the test data, we use the `recall_score()` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_per_class = recall_score(true_classes, predicted_classes, average = None)\n",
    "for label, recall in enumerate(recall_per_class):\n",
    "    print(f'Recall for digit {label}: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23815765",
   "metadata": {},
   "source": [
    "Let's interpret the results using the first line of the output as an example. \n",
    "\n",
    "The recall for the digit '0' in this case is 0.9816 or 98.16%. This means that out of all the images that were truly labeled as '0', the model correctly identified 98.16% of them, but it missed 1.84% of the actual '0' images (these were incorrectly classified as some other digit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d4504",
   "metadata": {},
   "source": [
    "Note that the recall metric for digit '0' can also be derived from the confusion matrix, where **TP** is **962** and **FN** is **18**, which is the sum of the misclassifications in the same row:\n",
    "\n",
    "<img src=\"recall.jpg\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e2eb9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>6. F1-Score</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67eb59",
   "metadata": {},
   "source": [
    "The **F1-score** is a metric that combines both precision and recall into a single number, providing a balanced measure of a model's performance. It is particularly useful when you want to evaluate a modelâ€™s performance in scenarios where there is a trade-off between precision and recall, or when you are dealing with imbalanced datasets.\n",
    "\n",
    "Mathematically, the F1-score is the harmonic mean of precision and recall and is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision}+\\text{Recall}}\n",
    "$$\n",
    "\n",
    "The F1-score is particularly important when:\n",
    "+ **Working with imbalanced datasets:** These are cases where one class is much more frequent than others (e.g., fraud detection, rare disease diagnosis). In these cases, accuracy can be misleading because the model may perform well by simply predicting the majority class.\n",
    "+ **Balancing between precision and recall:** When we need to balance the risk of false positives (precision) with the risk of false negatives (recall), the F1-score gives a single metric that reflects that balance.\n",
    "\n",
    "To calculate the f1-score of a model for each class of the test data, we use the `f1_score()` function from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d5ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedfc720",
   "metadata": {},
   "source": [
    "Let's interpret the results using the first line of the output as an example. \n",
    "\n",
    "An F1-score of 0.9872 (98.72%) means that the model has a good balance between precision and recall. It is performing well in terms of both minimizing false positives and correctly identifying true positives.\n",
    "\n",
    "A poor F1-score typically falls significantly lower, such as below 0.5, indicating that the model struggles to maintain a good balance between precision and recall. A low score suggests that a model either produces many false positives (low precision) or fails to identify a significant portion of true positives (low recall)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
